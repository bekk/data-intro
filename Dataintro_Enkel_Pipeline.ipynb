{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataintro Enkel Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Datapipeline introduksjon\n",
        "\n",
        "# Workshop: Introduksjon til dataflyt og transformasjon\n",
        "\n",
        "**Du vil l칝re:**\n",
        "- Helt overordnet hva dataflyt og transformasjon er, hva det inneb칝rer og hvordan det utf칮res i praksis\n",
        "- Litt om Google Cloud Storage og Google BigQuery\n",
        "- 칀 laste inn, hente ut og jobbe med data fra Google BigQuery\n",
        "- Gj칮re enkle transformasjoner ved hjelp av datamanipulerings verkt칮y\n",
        "- Lage et nytt og rikere datasett med data fra flere kilder \n",
        "\n",
        "**Du vil _ikke_ l칝re:**\n",
        "- Hvordan man setter opp en faktisk dataflyt eller lignende i Google Cloud Platform\n",
        "- Spesifikke detaljer om verkt칮y som Google Cloud Platform, Pandas, Matplotlib, Keras, Tensorflow og liknende"
      ],
      "metadata": {
        "id": "raflZ3eh1f4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Noen utvalgte Google skykomponenter\n",
        "#### Storage Bucket (Google Cloud Storage)\n",
        "\n",
        "En storage bucket er fin 친 ha n친r man 칮nsker 친 lagre store mengder med r친data, eller ustrukturerte data. Hvis man har f.eks CSV uttrekk, kan disse lastes opp i en cloud storage.\n",
        "\n",
        "Cloud Storage er ogs친 fin 친 bruke som et landingsomr친de for dataene dine. F.eks hvis du henter data via et API, k칮/streaming eller batch, men ikke vet hvordan datastrukturene dine ser ut, kan du lagre de p친 Cloud Storage. P친 denne m친ten trenger du ikke 친 v칝re avhengig av kildesystemet hvor dataene stammaer fra for 친 jobbe videre med disse.\n",
        "\n",
        "En av fordelene med Google Cloud Storage er at det er billig 친 lagre store mengder data.\n",
        "\n",
        "Det er derimot ikke s친 lett 친 lese innholdet i filene direkte fra en storage bucket. Vi 칮nsker derfor 친 flytte disse et sted hvor det er lett 친 analysere. Et slikt verkt칮y kan v칝re Google BigQuery, som er en database tilpasset analyse. Dette kan videre kobles opp mot visualiseringsverkt칮y som Google Data Studio eller Google Colab notebooks (ML).\n",
        "\n",
        "#### Google BigQuery\n",
        "\n",
        "BigQuery er en SQL basert database som er optimalisert for analyse. I motsetning til tradisjonelle SQL servere, er den kolonnebasert (ikke radbasert). Dette medf칮rer at den er veldig rask p친 친 regne ut aggregerte tall, som er supert for analyseform친l. Den takler ogs친 brede tabeller veldig godt, slik at man kan ha mange flagg/attributter per rad."
      ],
      "metadata": {
        "id": "DPfwNsUoPddk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Oppgave 1 - Google Cloud Storage\n",
        "I Google Cloud Console (GUI) for prosjektet (data-intro), finn Google Cloud Storage. Her finner du en bucket med to ulike datasett. Hva finner du ut om disse datasettene (metadata)?\n",
        "- Hvilken filtype er de?\n",
        "- Hvor store er filene?\n",
        "- Annen informasjon?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wQ3gSFzt9Dnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Oppgave 2 - BigQuery\n",
        "I Google Cloud Storage kan vi se \"data om data(settet)\" v친rt, s친kalt metadata. Men det er vanskelig 친 se hva som faktisk ligger inne i datasettet. Dette er ikke lett 친 se fra en Storage Bucket, s친 vi 칮nsker 친 flytte dataen til et annet verkt칮y.\n",
        "\n",
        "\n",
        "Vi 칮nsker 친 flytte datasettene til BigQuery for 친 kunne se innholdet.\n",
        "\n",
        "1. Finn BigQuery i menyen\n",
        "2. Velg prosjektet \"Data intro\" og deretter marker datasettet bysykkel_main\n",
        "3. I menylinjen oppe til h칮yre, velg \"Create table\". Kall den nye tabellen din bysykkel_(gruppenavn)\n",
        "4. Her kan du velge datakilden din. Vi 칮nsker 친 velge oslobysykkel datasettet fra Storage Bucket. Filformatet fant du i oppgave 1. Fyll inn informasjon om \"destination\". La BigQuery definere skjema for deg, og behold ellers standard innstillingene.\n",
        "\n",
        "Datasettet er n친 lastet inn i BigQuery. BigQuery har en preview funksjon, i tillegg til muligheten til 친 sp칮rre om data ved SQL-sp칮rringer. Hva finner du ut om skjema og innholdet?"
      ],
      "metadata": {
        "id": "9tCc63aZWUgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "#### Oppgave 3 - BigQuery\n",
        "Gjenta det samme for det andre datasettet, kall tabellen v칝rdata_(gruppenavn).\n",
        "\n"
      ],
      "metadata": {
        "id": "cI5dylecWZnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Oppgave 4 - Utvidet datasett\n",
        "Vi 칮nsker 친 sl친 sammen de to datasettet slik at vi kan gj칮re analyse p친 tvers disse senere.\n",
        "\n",
        "N친r vi sl친r sammen data gj칮r vi en *transformasjon* p친 datasettet, det vil si vi gj칮r noen endringer p친 datasettet for 친 칮ke kvaliteten for analyse.\n",
        "\n",
        "Det finnes flere verkt칮y man kan benytte for dette, i denne workshopen bruker vil notebook-verkt칮yet Google Colab.\n",
        "\n",
        "游눠 **NB! Det fungerer ikke s친 bra 친 redigere og/eller kj칮re kode i Colab samtidig med andre brukere. Det er derfor viktig at du jobber i din egen notebook, og ikke i denne hvor oppgavene st친r.**\n",
        "\n",
        "1. Opprett en Colab notebook. Du kan kalle denne hva du vil. Eventuelt kan du lage en kopi av denne notebooken.\n",
        "2. Koble deg til BigQuery:\n",
        "(Kopier kodesnuttene under inn i din notebook).\n",
        "  - F칮rst m친 du autentisere deg (med din Bekk Google bruker som har tilgang til BigQuery)\n"
      ],
      "metadata": {
        "id": "s9nSGug2NrRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate your Google Account\n",
        "# Doing so means you have access to various\n",
        "# resources connected to your account, such\n",
        "# as BigQuery tables, Storage buckets etc.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "eqEVQIJsT7K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3.  N친r du er autentisert, laster du inn bysykkel datasettet og v칝rdatasettet til gruppen din inn i notebooken deres. Datasettene blir lastet inn som dataframes ( les mer om Dataframes her https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). Du kan kalle dataframesene dine henholdsvis `df_bysykkel` og `df_weather`.\n",
        "\n",
        "<details><summary>游뚿 L칮sningsforslag</summary>\n",
        "\n",
        "Husk 친 kj칮re kodesnutten med importene og hjelpefunksjonen f칮r du kj칮rer l칮sningsforslaget under:\n",
        "\n",
        "```# L칮sning \n",
        "df = load_bigquery_data(\"data-intro\", \"bysykkel_main\", \"bysykkel_[DITT_GRUPPENAVN]\")\n",
        "df.head()\n",
        "```\n",
        "```\n",
        "df_weather = load_bigquery_data(\"data-intro\", \"bysykkel_main\", \"v칝rdata_joakim\")\n",
        "df_weather.head()\n",
        "```\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "jyJPs0v4T-35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HUSK 칀 KOPIER DENNE KODEBLOKKEN TIL NOTEBOOKEN DIN, OG KJ칒R DEN FOR 칀 F칀 TILGANG TIL IMPORTENE OG HJELPEFUNKSJONEN\n",
        "# Importer eksterne avhengigheter\n",
        "from google.cloud import bigquery_storage, bigquery\n",
        "from google.cloud.bigquery_storage import types\n",
        "from google.cloud.bigquery_storage_v1 import enums\n",
        "import pandas\n",
        "\n",
        "# Input:\n",
        "#   project_id: string\n",
        "#   dataset_id: string\n",
        "#     table_id: string\n",
        "# \n",
        "# Output:\n",
        "#   dataframe: pandas dataframe\n",
        "#\n",
        "\n",
        "# Hjelpemetode for 친 laste inn datasettet ditt. Du kan bruke denne for 친 laste inn datasettene dine\n",
        "def load_bigquery_data(project_id, dataset_id, table_id):\n",
        "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
        "    table = f\"projects/{project_id}/datasets/{dataset_id}/tables/{table_id}\"\n",
        "    parent = \"projects/{}\".format(project_id)\n",
        "\n",
        "    # Opprett en read-session mot en tabell i BigQuery\n",
        "    requested_session = types.ReadSession(\n",
        "        table=table,\n",
        "        data_format = enums.DataFormat.ARROW\n",
        "    )\n",
        "    read_session = bqstorageclient.create_read_session(\n",
        "        parent=parent,\n",
        "        read_session=requested_session,\n",
        "        max_stream_count=1,\n",
        "    )\n",
        "\n",
        "    # Les data fra BigQuery, putt i en liste med \"frames\"\n",
        "    # og kombin칠r til 칠n enkelt Pandas DataFrame\n",
        "    stream = read_session.streams[0]\n",
        "    reader = bqstorageclient.read_rows(stream.name)\n",
        "    frames = []\n",
        "    for message in reader.rows(read_session).pages:\n",
        "        frames.append(message.to_dataframe())\n",
        "    dataframe = pandas.concat(frames)\n",
        "\n",
        "    return dataframe"
      ],
      "metadata": {
        "id": "AXXkkVCBUDEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Vi m친 finne en kolonne med fellesdata for 친 kunne sl친 de sammen (joine) tabellene. S친 du noen fellesnevnere da du unders칮kte innholdet i tabellene?\n",
        "\n",
        "4. Et alternativ er 친 sl친 sammen tabellene basert p친 dato. V칝rdata har en entry per rad basert p친 dato. P친 bysykkel datasettet har vi flere kolonner som inneholder dato, s친 her m친 vi velge en. Valget avhenger av hva vi 칮nsker 친 analysere. Dette datasettet er ikke s친 omfattende, s친 i v친rt eksempel velger vi 친 joine p친 turens starttidspunkt (`started_at`).\n",
        "\n",
        "- Sl친 sammen tabellene ved 친 bruke `started_at` i bysykkeldatasettet og `date` i v칝rdatasettet.\n",
        "\n",
        "  - 游눠  **Tips:** [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.merge.html) funksjonen i pandas kan hjelpe deg med 친 sl친 sammen datasett.\n",
        "\n",
        "\n",
        "<details><summary>游뚿 L칮sningsforslag</summary>\n",
        "Vi kan sl친 sammen datasett ved 친 bruke `merge`, og fortelle funksjonen hvilke kolonner som skal sl친s sammen, samt hvilken type join vi 칮nsker (f.eks left, right, outer etc.)\n",
        "\n",
        "```\n",
        "  df_2 = pandas.merge(df, df_weather, on=['started_at', 'date'], how='left') \n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "   \n",
        "St칮tte du p친 en utfordring og fikk en feilmelding her? Hvorfor fungerer ikke dette?\n",
        "\n",
        "<details><summary>游뚿 L칮sningsforslag</summary>\n",
        "\n",
        "Ta en kikk p친 innholdet i disse to kolonnene. Ser det ut som de er det samme i de ulike tabellene? Her m친 vi gj칮re mer transformasjon f칮r vi kan fortsette!\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFxke3DCTkmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Oppgave 5 - Utvidet datasett 2\n",
        "Transformasjoner er en stor og viktig del n친r vi jobber med data. Ofte er datasettene vi har til r친dighet ikke p친 det formatet vi 칮nsker 친 ha det p친. 칀 transformere data betyr 친 gj칮re endringer (f.eks sl친 sammen datasett, endre p친 datatyper, fjerne duplikater, gj칮re utregninger med basis i andre kolonner, eller fjerne potensielle \"outliers\" som kan 칮delegge grunnlaget v친rt for analyse)\n",
        "\n",
        "> 游빞 Dette kalles ofte for 친 vaske data.\n",
        "\n",
        "Vi m친 f친 datokolonnene til 친 v칝re p친 samme format. En m친te vi kan gj칮re dette p친 er 친 kun bruke dato delen av `started_at`. Ulempen med dette er at vi da mister informasjon vi kanskje 칮nsker 친 bruke videre i analyse/innsiktsdelen, i dette tilfellet ville vi mistet klokkeslettet. \n",
        "\n",
        "Vi l칮ser dette problemet med 친 lage en hjelpekolonne, alts친 en ny midlertidg kolonne som kun brukes n친r vi sl친r sammen datasettene\n",
        "\n",
        "1. Lag en ny kolonne, `trip_date`, som kun inneholder datoen fra kolonnen `started_at`.\n",
        "\n",
        "**游눠 Tips**: Pandas har en funksjon [`to_datetime`](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) som lar deg tilpasse tidspunkter\n",
        "\n",
        "<details><summary>游뚿 L칮sningsforslag</summary>\n",
        "\n",
        "```\n",
        "df[\"trip_date\"] = pandas.to_datetime(df[\"started_at\"]).dt.strftime(\"%Y-%m-%d\") \n",
        "df.head()\n",
        "\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "\n",
        "2. Fungerer det 친 merge n친? \n",
        "  - Mest sannsynlig ikke. Hvorfor ikke? \n",
        "\n",
        "<details>\n",
        "<summary>游뚿 L칮sningsforslag</summary>  \n",
        "Sjekk datatypene i begge dataframene. Kall `d_types` (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html). Her f친r du opp hvilke datatyper kolonnene er (det kan v칝re strings, integers, objects ++) Mest sannsynlig best친r n친 kolonnene av object-typer.\n",
        "\n",
        "</details>\n",
        "\n",
        "3. For 친 fortsette m친 kolonnene ha samme datatype for 친 kunne sl친s sammen. Gj칮r om `trip_date` i bysykkeldatasettet og `date` i v칝rdatasettet til datetime typer.\n",
        "\n",
        "<details>\n",
        "<summary>游뚿 L칮sningsforslag</summary>  \n",
        "```python\n",
        "df[\"trip_date\"] = pandas.to_datetime(df[\"trip_date\"])\n",
        "df_weather[\"date\"] = pandas.to_datetime(df_weather[\"date\"])```\n",
        "\n",
        "</details>\n",
        "\n",
        "4. Merge datasettet\n",
        "\n",
        "<details>\n",
        "<summary>游뚿 L칮sningsforslag</summary>  \n",
        "```\n",
        "df_merged = df.merge(df_weather, left_on='trip_date', right_on='date', how='left')\n",
        "df_merged.head()```\n",
        "\n",
        "</details>\n",
        "\n",
        "5. Kj칮r `d_types`. V칝rdata-kolonnene ser ikke ut til 친 v칝re av typen tall. Endre datatypen p친 denne slik at det blir desimaltall\n",
        "\n",
        "<details>\n",
        "<summary>游뚿 L칮sningsforslag</summary>  \n",
        "Kolonnen ser tilsynelatende ut til 친 kun best친 av tall. Hvis vi inspiserer verdiene, ser vi at noen ganger forekommer strengen \"NULL\". Vi m친 rydde opp i disse f칮r vi kan endre datatypen til float. Vi kan bruke `replace` for dette.\n",
        "\n",
        "```\n",
        "df_merged[\"mean_temperature\"] = df_merged[\"mean_temperature\"].replace('NULL',0).astype('float')\n",
        "df_merged[\"precipitation_amount\"] = df_merged[\"precipitation_amount\"].replace('NULL',0).astype('float')\n",
        "\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "6. Rydd opp i dataframen ved 친 fjerne  hjelpekolonnen\n",
        "\n",
        "<details>\n",
        "<summary>游뚿 L칮sningsforslag</summary>  \n",
        "```\n",
        "df_merged = df_merged.drop(columns=\"date\")\n",
        "df_merged.head()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "Vi har n친 et utvidet datasett! 游꿀游볙游꿁\n",
        "\n",
        "\n",
        "Vanligvis ville vi ha skrevet datasettet tilbake til BigQuery, men det gj칮r vi ikke i denne workshopen. (Det tok lang tid, vi har mye data游땺) \n",
        "\n",
        "Pandas har en ganske snedig funksjon som kan gj칮re dette!\n",
        "\n",
        "```df_merged.to_gbq(\"bysykkel_main.bysykkel_med_v칝rdata\", project_id=\"data-intro\")```\n",
        "\n",
        "N친r vi skriver til BigQuery, vil datatypene i dataframen f칮lge med og sette riktig skjema i BigQuery\n"
      ],
      "metadata": {
        "id": "vUZMf3ycUY7D"
      }
    }
  ]
}